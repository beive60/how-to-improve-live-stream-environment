# 音声制作パイプライン タスクリスト

channel-growth-strategy.md Section 9.1 の実行タスク。
RVC v2 による自己音声モデルの構築から、動画制作での運用までを網羅する。

## Phase 1: 環境構築

### 1.1 RVC v2 のインストール

- [ ] RVC v2 リポジトリをクローンする（Retrieval-based-Voice-Conversion-WebUI）
- [ ] Python 3.10.x の仮想環境を作成する（3.11以降は依存関係の互換性問題あり）
- [ ] `pip install -r requirements.txt` で依存パッケージをインストールする
- [ ] CUDA Toolkit のバージョンが PyTorch と一致していることを確認する
- [ ] WebUI を起動し、ブラウザからアクセスできることを確認する（`python infer-web.py`）
- [ ] 事前学習済みモデル（pretrained_v2）が `assets/pretrained_v2/` に存在することを確認する

### 1.2 収録環境の準備

- [ ] 収録場所の暗騒音レベルを計測する（目標: -60dBFS以下）
- [ ] マイクのポジションを固定する（口元から15-20cm、ポップフィルター使用）
- [ ] オーディオIFの設定を確認する（サンプリングレート: 48kHz、ビット深度: 24bit）
- [ ] DAWまたはAudacityで収録テストを行い、ノイズフロアを確認する
- [ ] 録音レベルを調整する（ピーク: -6dBFS〜-3dBFS）

## Phase 2: データセット作成

### 2.1 音声収録

- [ ] 朗読用テキストを準備する（技術系の文章を推奨。本番のナレーションに近いトーンで）
- [ ] 15-30分の朗読音声を一括収録する（途中で休憩可。同一セッション内で完結させる）
- [ ] 収録時の注意事項:
  - 一定の声量とテンポを維持する
  - 感情の起伏は抑え、ニュートラルなトーンで読む
  - 噛んだ箇所は一呼吸おいて読み直す（後で切り出すため問題ない）
  - 水分を適宜補給し、口内の乾燥を防ぐ

### 2.2 前処理

- [ ] 収録音声をWAV形式（48kHz / 24bit モノラル）で書き出す
- [ ] Audacity等で以下の処理を行う:
  - 無音区間、噛み箇所、ノイズ混入箇所を除去する
  - ブレス音、リップノイズを手動で除去またはゲート処理する
  - DC Offsetがあれば除去する
- [ ] 2-10秒単位のクリップに分割する（RVCの学習に最適な長さ）
- [ ] 分割後のクリップを `dataset/` フォルダに格納する
- [ ] 合計の有効音声時間が15分以上あることを確認する

## Phase 3: モデルトレーニング

### 3.1 学習の実行

- [ ] RVC WebUIの「Train」タブを開く
- [ ] 以下のパラメータを設定する:

| パラメータ | 推奨値 | 説明 |
| --- | --- | --- |
| Experiment Name | 自分の識別名（例: `my_voice_v01`） | モデルの保存名 |
| Target Sample Rate | 48k | 収録音声のサンプリングレートに合わせる |
| Model Architecture | v2 | v1より音質が向上している |
| Pitch Guidance | true | 音程情報を学習に含める。音声の自然さが向上する |
| Training Dataset Path | `dataset/` フォルダのパス | 前処理済みクリップの格納先 |
| Batch Size | 8-16（VRAM依存） | VRAM 8GB: 8、12GB: 12、24GB: 16 |
| Epochs | 100-200 | 100エポックで一旦確認し、品質不足なら追加学習 |
| Save Frequency | 25 | 25エポックごとにチェックポイントを保存 |

- [ ] 「Process Data」ボタンで前処理を実行する（ピッチ抽出、特徴量抽出）
- [ ] 「Train Model」ボタンで学習を開始する
- [ ] 「Train Feature Index」ボタンでインデックスファイルを生成する
- [ ] 学習完了後、以下のファイルが生成されていることを確認する:
  - `.pth` ファイル（モデル本体）: `logs/<experiment_name>/` 配下
  - `.index` ファイル（特徴量インデックス）: 同フォルダ配下

### 3.2 学習品質の検証

- [ ] 25エポック刻みの各チェックポイントで推論テストを行い、最適なエポック数を特定する
- [ ] 過学習の兆候を確認する:
  - 声が機械的・金属的に聞こえる → 過学習。エポック数を減らす
  - 元の声との差が大きすぎる → 過学習。早い段階のチェックポイントを採用する
  - 子音が不明瞭になる → 過学習。protect値を上げて再推論する

## Phase 4: 推論（モデル使用）の詳細手順

ここからが「トレーニング済みモデルを実際のナレーション制作に使う」工程となる。

### 4.1 推論の基本概念

RVC推論は以下の処理を行う:

```
[入力] 自分の声で収録したナレーション音声（WAV）
          |
          v
    RVC v2 推論エンジン
    - 入力音声のピッチ（f0）を抽出する
    - 入力音声の特徴量を抽出する
    - 学習済みモデルで特徴量を変換する
    - インデックスファイルで音色の類似度を補正する
    - 変換後の波形を生成する
          |
          v
[出力] 声質が均一化されたナレーション音声（WAV）
```

入力と出力で「話している内容」は一切変わらない。変わるのは声質の一貫性と音響特性のみ。

### 4.2 WebUIでの推論手順

1. RVC WebUIの「Inference」タブを開く
2. 「Model」ドロップダウンから学習済みモデル（.pth）を選択する
3. 「Index File」から対応する .index ファイルを選択する
4. 「Audio File」に変換したいナレーション音声のパスを入力する
5. 推論パラメータを設定する（次のセクション参照）
6. 「Convert」ボタンを押す
7. 出力音声が `opt/` フォルダに生成される

### 4.3 推論パラメータの詳細

各パラメータの意味と推奨設定値。自己音声の品質補正用途に最適化した値を記載する。

| パラメータ | 推奨値 | 範囲 | 説明 |
| --- | --- | --- | --- |
| Transpose (f0 up key) | 0 | -12 ~ +12 | ピッチシフト（半音単位）。自分の声を自分のモデルに通すため、0で固定する |
| f0 Method | rmvpe | crepe / rmvpe / harvest / pm | ピッチ抽出アルゴリズム。rmvpeが精度と速度のバランスが最も良い |
| Search Feature Ratio | 0.5 - 0.75 | 0.0 - 1.0 | インデックスファイルの影響度。高いほど学習データの音色に寄せるが、上げすぎると不自然になる |
| Filter Radius | 3 | 0 - 7 | ピッチカーブの平滑化。3で自然なイントネーションを維持しつつブレを低減する |
| Resample Rate | 48000 | 0 / 任意 | 出力サンプリングレート。0で入力と同一。動画制作のパイプラインに合わせて48000を指定する |
| Volume Envelope (rms mix rate) | 0.25 | 0.0 - 1.0 | 音量エンベロープの混合比。0.0=変換後の音量をそのまま使用、1.0=入力の音量カーブを完全に維持。0.25で自然な音量感を保つ |
| Protect | 0.33 - 0.5 | 0.0 - 0.5 | 無声子音（s, t, k等）とブレス音の保護。高いほど子音がクリアになる。0.33で開始し、子音が不明瞭なら0.5に上げる |

### 4.4 パラメータ調整の判断基準

推論結果を聴いて以下の問題が発生した場合の対処方法。

| 症状 | 原因 | 対処 |
| --- | --- | --- |
| 声が機械的/ロボット的 | Search Feature Ratioが高すぎる | 0.5まで下げる |
| 子音（さ行、た行等）が不明瞭 | Protectが低い | 0.5に上げる |
| ピッチが不安定に揺れる | f0 Methodの精度不足 | rmvpeまたはcrepeに変更する |
| 音量が不自然に変動する | Volume Envelopeの設定不適切 | rms mix rateを0.5に上げて入力の音量カーブを維持する |
| 息遣いが消える | Protectが低い / Search Feature Ratioが高い | Protectを0.5に、Search Feature Ratioを0.5に |
| 元の声とほとんど変わらない | Search Feature Ratioが低すぎる | 0.75に上げる。効果が薄い場合はモデルの品質を見直す |
| ノイズが増幅される | 入力音声のノイズが変換で強調された | 入力音声の前処理を見直す（ノイズゲート、ノイズリダクション） |

### 4.5 CLIでのバッチ推論（自動化）

毎回WebUIを操作するのは非効率なため、CLIでのバッチ処理を構築する。

```
ナレーション収録（セクション単位）
    |
    v
前処理（ノイズ除去、レベル調整）
    |
    v
RVC CLI バッチ推論（全クリップを一括変換）
    |
    v
後処理（ノーマライズ、EQ、コンプレッション）
    |
    v
動画編集ソフトへインポート
```

**CLIコマンドの基本構造**

```bash
python infer_cli.py \
  --model_path "logs/my_voice_v1/my_voice_v1.pth" \
  --index_path "logs/my_voice_v1/added_my_voice_v1.index" \
  --input_path "input/" \
  --output_path "output/" \
  --f0_method rmvpe \
  --transpose 0 \
  --index_rate 0.6 \
  --filter_radius 3 \
  --resample_sr 48000 \
  --rms_mix_rate 0.25 \
  --protect 0.4
```

- [ ] 上記コマンドをバッチスクリプト（.bat / .ps1）として保存する
- [ ] `input/` と `output/` のフォルダパスを動画ごとに切り替えられるように変数化する
- [ ] 推論完了後に自動で出力フォルダを開くステップを追加する

### 4.6 推論後の後処理

RVC出力はそのままでは動画に使えない場合がある。以下の後処理を施す。

- [ ] ラウドネスノーマライズ: 全クリップを -16 LUFS（YouTube推奨値）に統一する
- [ ] EQ: 100Hz以下をハイパスフィルタでカットする（環境ノイズの残留を除去）
- [ ] コンプレッション: ナレーション音声の音量差を圧縮する（レシオ 3:1、スレッショルド -18dB 程度）
- [ ] リミッター: トゥルーピーク -1.0dBTP以下に制限する
- [ ] 無音区間の調整: セクション間のポーズ長を統一する（0.5-1.0秒）

## Phase 5: 動画制作への統合

### 5.1 ナレーション収録から完成までのワークフロー

各動画の制作において、以下の手順で音声を制作する。

| Step | 作業 | 入力 | 出力 | 所要時間（目安） |
| --- | --- | --- | --- | --- |
| 1 | 台本作成 | 動画の構成案 | ナレーション台本（テキスト） | 2-4時間 |
| 2 | ナレーション収録 | 台本 | 生の収録音声（WAV） | 1-2時間（10-15分動画分） |
| 3 | 収録音声の編集 | 生の収録音声 | 噛み/ミス除去済み音声 | 1-2時間 |
| 4 | RVC推論 | 編集済み音声 | 声質均一化済み音声 | 10-30分（自動処理） |
| 5 | 後処理 | RVC出力 | 最終ナレーション音声 | 30分-1時間 |
| 6 | 動画編集へ統合 | 最終ナレーション音声 | タイムライン上のナレーショントラック | - |

### 5.2 ファイル命名規則

```
EP001/
  audio/
    raw/          -- Step 2: 生の収録音声
      EP001_sec01_raw.wav
      EP001_sec02_raw.wav
    edited/       -- Step 3: 編集済み音声
      EP001_sec01_edited.wav
      EP001_sec02_edited.wav
    rvc/          -- Step 4: RVC推論出力
      EP001_sec01_rvc.wav
      EP001_sec02_rvc.wav
    final/        -- Step 5: 後処理済み最終音声
      EP001_sec01_final.wav
      EP001_sec02_final.wav
```

### 5.3 品質チェックリスト（毎動画）

- [ ] RVC出力と元音声をA/B比較し、声質の一貫性が向上していることを確認する
- [ ] 子音の明瞭度が維持されていることを確認する（特に「さ行」「た行」「は行」）
- [ ] 不自然なアーティファクト（金属音、途切れ、ピッチの飛び）がないことを確認する
- [ ] ラウドネスが -16 LUFS に収まっていることを計測する
- [ ] トゥルーピークが -1.0dBTP を超えていないことを確認する

## Phase 6: モデルの保守

### 6.1 モデルの更新判断基準

以下の場合にモデルの再学習を検討する。

- マイクまたはオーディオIFを変更した場合（音響特性が変わるため）
- 既存モデルの推論結果に一貫して不満がある場合
- 6ヶ月以上経過し、自分の声質が変化したと感じる場合

### 6.2 バックアップ

- [ ] 学習済みモデル（.pth）をクラウドストレージにバックアップする
- [ ] インデックスファイル（.index）を同様にバックアップする
- [ ] 学習パラメータの設定値をこのドキュメントまたは設定ファイルに記録する
- [ ] データセット（収録音声クリップ）もバックアップする（再学習に必要）
